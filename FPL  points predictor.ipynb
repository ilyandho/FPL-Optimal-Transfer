{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor as xgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# For the linear model\n",
    "\n",
    "\n",
    "def Linear_regression(features_train, features_test, target_train, target_test):\n",
    "    # Before using our data, we need to do feature scaling and we opt for the 'standardization' method of scaling.\n",
    "    # The 'standardization' is avaliable thorugh the StandardScaler() method\n",
    "    # Transformers help in batching tasks in a pipepline. In this case, the data is scaled and then a linear regression model is fitted on the scaled data.\n",
    "    # We use a transformer that takes the regression model and the transformation method\n",
    "    # The TransformedTargetRegressor does the transformation and when we do the prediction, it automatically does the inverse transformation (scaling) and returns the values\n",
    "    model = TransformedTargetRegressor(\n",
    "        LinearRegression(), transformer=StandardScaler())\n",
    "\n",
    "    # fit the transofrmer on the train data\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    # With the model fitted, we can predict the total_points given the feature_train and feature_test set\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    # Evaluate the performance of the model on both sets using the root mean square error\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    # Get the score of the model or the coeeficient of determination i.e how much of the target value can be explained by the model.\n",
    "    # In this case, 0.6 implies that 60% of the variations in the target value can be explained by the model and 40% is not explainable\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    # If the test error significantly differs from the train error, then there is either overfitting or underfitting\n",
    "    # RMSE, just like the squared loss function that it derives from, effectively penalizes larger errors more severely.\n",
    "    print('Training set RMSE: {}'.format(train_RMSE))\n",
    "    print('Test set RMSE: {}'.format(test_RMSE))\n",
    "\n",
    "\n",
    "\n",
    "    print('Training set R2: {}'.format(R2_train))\n",
    "    print('Test set R2: {}'.format(R2_test))\n",
    "\n",
    "    # Carry out cross validation of the model.\n",
    "    # The evaluation method is the root mean square error\n",
    "    # The method expects a utility function (greater is better) and so the scoring function is the opposite of the the RMSE. Hence the -ve\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "    print(pd.Series(tree_rmses).describe())\n",
    "\n",
    "    return {'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "\n",
    "# Decision Tree Model\n",
    "def DecisionTreeRegression(features_train, features_test, target_train, target_test):\n",
    "    # The DecisionTreeRegressor is passed as the model to the TransformedTreeRegressor together with the StandardScaler\n",
    "    model = TransformedTargetRegressor(\n",
    "        DecisionTreeRegressor(), transformer=StandardScaler())\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    print('Training set RMSE: {}'.format(train_RMSE))\n",
    "    print('Test set RMSE: {}'.format(test_RMSE))\n",
    "    print('Training set R2: {}'.format(R2_train))\n",
    "    print('Test set R2: {}'.format(R2_test))\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "    print(pd.Series(tree_rmses).describe())\n",
    "    return {'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "\n",
    "# RandomForestRegressor\n",
    "def RandomForestRegression(features_train, features_test, target_train, target_test, hyperparameters):\n",
    "    # RandomForestRegressor is an ensemble method\n",
    "    # The TransformedTargetRegressor is passed the RandomForestRegressor model\n",
    "    # The RandomForestRegressor is passed some hyper-parameters such as;\n",
    "    # n_esimtaors: number of trees in the forest,\n",
    "    # max_depth: the maximum depth of the tree,\n",
    "    # criterion: the function to measure the quality of the split\n",
    "\n",
    "    model = TransformedTargetRegressor(RandomForestRegressor(\n",
    "        n_estimators=hyperparameters['n_estimators'],  max_depth=hyperparameters['max_depth'], criterion=hyperparameters['criterion'], random_state=18), transformer=StandardScaler())\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train, pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    # print('Training set RMSE: {}'.format(train_RMSE))\n",
    "    # print('Test set RMSE: {}'.format(test_RMSE))\n",
    "    # print('Training set R2: {}'.format(R2_train))\n",
    "    # print('Test set R2: {}'.format(R2_test))\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train,\n",
    "                                  scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "    print(pd.Series(tree_rmses).describe())\n",
    "\n",
    "    return {'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "\n",
    "def XGBoostRegression(features_train, features_test, target_train, target_test, hyperparameters):\n",
    "    regressor = xgb(learning_rate=hyperparameters[\"learning_rate\"],\n",
    "                    n_estimators=hyperparameters[\"n_estimators\"],\n",
    "                    max_depth=hyperparameters[\"max_depth\"],\n",
    "                    eval_metric='rmsle')\n",
    "\n",
    "    model = TransformedTargetRegressor(regressor, transformer=StandardScaler())\n",
    "\n",
    "\n",
    "    model.fit(features_train, target_train)\n",
    "\n",
    "    # =========================================================================\n",
    "    # To use early_stopping_rounds:\n",
    "    # \"Validation metric needs to improve at least once in every\n",
    "    # early_stopping_rounds round(s) to continue training.\"\n",
    "    # =========================================================================\n",
    "    # first perform a test/train split\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # X_train,X_test,y_train,y_test = train_test_split(X_train,y_train, test_size = 0.2)\n",
    "    # model.fit(X_train, y_train, early_stopping_rounds=6, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    # =========================================================================\n",
    "    # use the model to predict the prices for the test data\n",
    "    # =========================================================================\n",
    "    # predictions = model.predict(goalkeepers_splits['feature_test'])\n",
    "\n",
    "    pred_train = model.predict(features_train)\n",
    "    pred_test = model.predict(features_test)\n",
    "\n",
    "    train_RMSE = mean_squared_error(target_train,  pred_train, squared=False)\n",
    "    test_RMSE = mean_squared_error(target_test, pred_test, squared=False)\n",
    "\n",
    "    R2_train = model.score(features_train, target_train)\n",
    "    R2_test = model.score(features_test, target_test)\n",
    "\n",
    "    # print('Training set RMSE: {}'.format(train_RMSE))\n",
    "    # print('Test set RMSE: {}'.format(test_RMSE))\n",
    "    # print('Training set R2: {}'.format(R2_train))\n",
    "    # print('Test set R2: {}'.format(R2_test))\n",
    "\n",
    "    tree_rmses = -cross_val_score(model, features_train, target_train, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "    pd.Series(tree_rmses).describe()\n",
    "\n",
    "    return {'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE,  'cv_rmse': tree_rmses.mean(), 'R2_train': R2_train, 'R2_test': R2_test}\n",
    "\n",
    "\n",
    "def GridSearchParams(features_train, target_train):\n",
    "    # Instatiate the model\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    param_grid = {'n_estimators': [8, 10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "    # Define the possible values of the hyperparameter\n",
    "    grid = {\n",
    "        'n_estimators': [8, 10, 12, 14, 16, 18, 20, 200, 300, 400, 500],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [4, 5, 6, 7, 8],\n",
    "        'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
    "        'random_state': [18]\n",
    "    }\n",
    "\n",
    "    # Deine the model with cv=3 for a 3-fold cross validation\n",
    "    # GridSearchCV has the best_estimator_ parameter that returns the  estimator\n",
    "    # which gave highest score (or smallest loss if specified)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, grid, cv=3, scoring='neg_root_mean_squared_error')\n",
    "    grid_search.fit(features_train, target_train)\n",
    "\n",
    "    # Get the best param combination\n",
    "    print(grid_search.best_estimator_)\n",
    "\n",
    "    return {'train_RMSE': train_RMSE, 'test_RMSE': test_RMSE, 'R2_train': R2_train, 'R2_test': R2_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that splits and returns features_train, features_test, target_train, target_test\n",
    "\n",
    "def split_data(data):\n",
    "    # Store the 'total_points' target in the 'player_target' variable\n",
    "    # and the rest in the player_features variable\n",
    "    player_target = data['total_points']\n",
    "    player_features = data.drop(\"total_points\", axis=1)\n",
    "\n",
    "    # The train_test_split function splits the set into train and test sets while maintain the same data distribution over both sets.\n",
    "    # It takes the feature and target sets and reutrns the respective train and test sets\n",
    "    features_train, features_test, target_train, target_test = train_test_split(\n",
    "        player_features, player_target, test_size=0.2)\n",
    "\n",
    "    return {'feature_train': features_train, 'features_test': features_test, 'target_train': target_train, 'target_test': target_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./content/fpl_player_data_new.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Separate the week to be predicted\n",
    "##\n",
    "def get_details(gw):\n",
    "    # player_data = pd.DataFrame(get_player_data(gw))\n",
    "\n",
    "    # Separate next gw's data\n",
    "    next_gw_data = df[df['gw'] == gw]\n",
    "\n",
    "    # Drop this data from the rest of data\n",
    "    player_data = df.drop(df[df['gw'] >= gw].index)\n",
    "\n",
    "    return player_data #, next_gw_data\n",
    "\n",
    "\n",
    "details = get_details(39)\n",
    "details.head()\n",
    "\n",
    "details.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
